from django.db import models
import uuid

class base_model(models.Model):
    unique_id = models.UUIDField(default=uuid.uuid4, editable=False, unique=True)
    created_by = models.CharField(max_length=50, null=True)
    updated_by = models.CharField(max_length=50, null=True)
    created_at = models.DateTimeField(auto_now_add=True)
    updated_at = models.DateTimeField(auto_now=True)


    

# Audio file must be dowunloaded prior
class AudioFile(base_model):
    audio_id = models.CharField(max_length=50, unique=True)
    audio_file = models.FileField(upload_to="")
    file_size = models.PositiveIntegerField(null=True)
    duration = models.FloatField(null=True)


    def __str__(self):
        return self.unique_id
    
    
# confirm audio files are usable example long silence , volume , within selected specttogram    
class evaluated_audio_file(base_model):
    audio_id = models.OneToOneField(AudioFile, on_delete=models.CASCADE, related_name="evaluated_audio_file",null=True,unique=True)
    is_evaluated = models.BooleanField(default=False)
    is_evaluated_by = models.CharField(max_length=50, null=True)
    evaluation_date = models.DateTimeField(null=True)
    evaluation_notes = models.TextField(null=True)
    evaluation_score = models.FloatField(null=True)
    evaluation_status = models.CharField(max_length=50, null=True)
    evaluation_result = models.CharField(max_length=50, null=True)
    evaluation_result_notes = models.TextField(null=True)
    evaluation_result_score = models.FloatField(null=True)
    evaluation_result_status = models.CharField(max_length=50, null=True)
    evaluation_result_notes = models.TextField(null=True)


    
    
    
    
# This should be imported from a CSV file generated by the helpline 
class CaseRecord(base_model):
    audio_id= models.OneToOneField(AudioFile, on_delete=models.CASCADE, related_name="case_record",null=True,unique=True)
    date = models.DateTimeField()
    talk_time = models.TimeField()
    case_id = models.CharField(max_length=20)
    narrative = models.TextField()
    plan = models.TextField()
    main_category = models.CharField(max_length=100)
    sub_category = models.CharField(max_length=100)
    gbv = models.BooleanField()

    

    def __str__(self):
        return f"Case {self.case_id} - {self.main_category}"
    




# The records should be created by the automated process to determine the sight spectogram and the transcription
class evaluation_record(base_model):
    audio_id = models.OneToOneField(AudioFile, on_delete=models.CASCADE, related_name="evaluation_record",null=True,unique=True)
    date = models.DateTimeField()
    talk_time = models.TimeField()
    case_id = models.CharField(max_length=20)
    narrative = models.TextField()
    plan = models.TextField()
    main_category = models.CharField(max_length=100)
    sub_category = models.CharField(max_length=100)
    gbv = models.BooleanField()
    gender = models.CharField(max_length=10, choices=CaseRecord.GENDER_CHOICES)
    locale = models.CharField(max_length=10, choices=CaseRecord.LOCALE_CHOICES)
    true_transcription = models.TextField()
    true_transcription_gender = models.CharField(max_length=10, choices=CaseRecord.GENDER_CHOICES)
    true_transcription_locale = models.CharField

    
class AudioFileChunk(base_model):
    GENDER_CHOICES = [
        ("male", "Male"),
        ("female", "Female"),
        ("not_sure", "Not Sure"),
    ]

    LOCALE_CHOICES = [
        ("en", "English"),
        ("sw", "Swahili"),
        ("both", "Both"),
    ]

    chunk_file = models.FileField(upload_to="audio_chunks/") # Order of the chunk in the original file
    duration = models.FloatField(null=True)
    feature_text = models.TextField(blank=True, null=True)  # ✅ Holds ground-truth transcriptions
    gender = models.CharField(max_length=10, choices=GENDER_CHOICES, default="not_sure")  # ✅ Gender field
    locale = models.CharField(max_length=5, choices=LOCALE_CHOICES, default="both")  # ✅ Locale field
    is_evaluated = models.BooleanField(default=False)  # ✅ Tracks whether chunk is evaluated
    eveluation_count = models.PositiveIntegerField(default=0)
    trancribe_chunk = models.BooleanField(default=False)


    class Meta:
        unique_together = ('parent_audio', 'order')  # Prevent duplicate chunk orders
          

    class evaluation_results(models.Model):
        chunk = models.OneToOneField(AudioFileChunk, on_delete=models.CASCADE, related_name="evaluation_results")
        single_speaker = models.BooleanField(default=False)
        sepeaker_overlap = models.BooleanField(default=False)
        background_noise = models.BooleanField(default=False)
        prolonged_silence  = models.BooleanField(default=False)
        background_noise_level = models.FloatField(null=False)   
        Not_Speech_Rate = models.BooleanField(default=False)
        echo_noise = models.BooleanField(default=False)
        is_evaluated = models.BooleanField(default=False)
        is_evaluated_by = models.CharField(max_length=50, null=True)
        evaluation_date = models.DateTimeField(null=True)
        evaluation_notes = models.TextField(null=True)



